{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader,TensorDataset, Dataset\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_loader(data_dir):\n",
    "    filenames = [name for name in os.listdir(data_dir)]\n",
    "    batch_size = len(filenames)\n",
    "    out = torch.zeros(batch_size, 3, 256, 1024, dtype=torch.uint8)\n",
    "    for i, filename in enumerate(filenames):\n",
    "        out[i] = torchvision.io.read_image(os.path.join(data_dir, filename))\n",
    "    return out\n",
    "\n",
    "def find_folders_in_directory(directory):\n",
    "    \"\"\"\n",
    "    Finds all folders in the given directory.\n",
    "\n",
    "    :param directory: The directory to search for folders.\n",
    "    :return: A list of paths to folders found in the given directory.\n",
    "    \"\"\"\n",
    "    folders = []\n",
    "    # Walk through the directory\n",
    "    for entry in os.scandir(directory):\n",
    "        # Check if the entry is a directory\n",
    "        if entry.is_dir():\n",
    "            folders.append(entry.path)\n",
    "    return folders\n",
    "\n",
    "# Example usage:\n",
    "# Replace 'your_directory_path' with the path to the directory you want to search\n",
    "def load_all_img(data_dir,folder_name):\n",
    "    out = []\n",
    "    fnames = find_folders_in_directory(data_dir)\n",
    "    for fn in fnames:\n",
    "        dir = os.path.join(fn,folder_name)\n",
    "        dataf = img_loader(dir)\n",
    "        if len(out) == 0:\n",
    "            out = dataf\n",
    "        else:\n",
    "            out = torch.vstack((out, dataf))\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img=load_all_img('/home/lambda2/carla_garage/data/ll_dataset_2023_05_10/Routes_Town04_ll_Repetition0', 'rgb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4582, 3, 256, 1024])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4582, 3, 256, 1024])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_img=load_all_img('/home/lambda2/carla_garage/data/ll_dataset_2023_05_10/Routes_Town04_ll_Repetition0', 'depth')\n",
    "\n",
    "depth_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(depth_img[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividing the dataset into test and train dataset\n",
    "# test_train_ratio = 0.9\n",
    "# pos_imgs_train = pos_imgs[0:int(np.floor(test_train_ratio*len(pos_imgs)))]\n",
    "# pos_imgs_test = pos_imgs[int(np.floor(test_train_ratio*len(pos_imgs))):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_dataloader = DataLoader(dataset = TensorDataset(input_img, depth_img), batch_size=16, shuffle=True, num_workers=16)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super(UNet, self).__init__()\n",
    "        # Convolutional Block in Encoder Section at Level i: ei   --   Decoder Section: di\n",
    "        self.e01 = nn.Conv2d(in_channels= 3, out_channels=c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn01 = nn.BatchNorm2d(num_features=c)\n",
    "        self.e02 = nn.Conv2d(in_channels=c, out_channels=c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn02 = nn.BatchNorm2d(num_features=c)\n",
    "        \n",
    "        self.e11 = nn.Conv2d(in_channels=c, out_channels=2*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn11 = nn.BatchNorm2d(num_features=2*c)\n",
    "        self.e12 = nn.Conv2d(in_channels=2*c, out_channels=2*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn12 = nn.BatchNorm2d(num_features=2*c)\n",
    "        \n",
    "        self.e21 = nn.Conv2d(in_channels=2*c, out_channels=4*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn21 = nn.BatchNorm2d(num_features=4*c)\n",
    "        self.e22 = nn.Conv2d(in_channels=4*c, out_channels=4*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn22 = nn.BatchNorm2d(num_features=4*c)\n",
    "        \n",
    "        self.e31 = nn.Conv2d(in_channels=4*c, out_channels=8*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn31 = nn.BatchNorm2d(num_features=8*c)\n",
    "        self.e32 = nn.Conv2d(in_channels=8*c, out_channels=8*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn32 = nn.BatchNorm2d(num_features=8*c)\n",
    "        \n",
    "        \n",
    "        self.d21 = nn.Conv2d(in_channels=8*c, out_channels=4*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn23 = nn.BatchNorm2d(num_features=4*c)\n",
    "        self.d22 = nn.Conv2d(in_channels=4*c, out_channels=4*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn24 = nn.BatchNorm2d(num_features=4*c)\n",
    "        \n",
    "        self.d11 = nn.Conv2d(in_channels=4*c, out_channels=2*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn13 = nn.BatchNorm2d(num_features=2*c)\n",
    "        self.d12 = nn.Conv2d(in_channels=2*c, out_channels=2*c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn14 = nn.BatchNorm2d(num_features=2*c)\n",
    "        \n",
    "        self.d01 = nn.Conv2d(in_channels=2*c, out_channels=c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn03 = nn.BatchNorm2d(num_features=c)\n",
    "        self.d02 = nn.Conv2d(in_channels=c, out_channels=c, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn04 = nn.BatchNorm2d(num_features=c)\n",
    "        \n",
    "        # Increaseing the resolution of image from a lower level to match the upper level\n",
    "        self.upconv3 = nn.ConvTranspose2d(in_channels=8*c, out_channels=4*c, kernel_size=2, stride=2)\n",
    "        self.upconv2 = nn.ConvTranspose2d(in_channels=4*c, out_channels=2*c, kernel_size=2, stride=2)\n",
    "        self.upconv1 = nn.ConvTranspose2d(in_channels=2*c, out_channels=c, kernel_size=2, stride=2)\n",
    "        ### Fully connected layer\n",
    "        self.fc1 = nn.Linear(40960, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 3)\n",
    "        self.softmax = nn.Softmax(dim=3)\n",
    "\n",
    "        # Output\n",
    "        self.out = nn.Conv2d(in_channels=c, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sig=nn.Sigmoid()\n",
    "        self.sm = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encoder\n",
    "        ### x0: output encoder0 \n",
    "        x0 = self.relu(self.bn02(self.e02(self.relu(self.bn01(self.e01(x))))))\n",
    "        x = self.maxpool(x0)\n",
    "        ### x1: output encoder 1\n",
    "        x1 = self.relu(self.bn12(self.e12(self.relu(self.bn11(self.e11(x))))))\n",
    "        x = self.maxpool(x1)\n",
    "        \n",
    "        ### x2: output encoder 2\n",
    "        x2 = self.relu(self.bn22(self.e22(self.relu(self.bn21(self.e21(x))))))\n",
    "        x = self.maxpool(x2)\n",
    "        \n",
    "        x = self.relu(self.bn32(self.e32(self.relu(self.bn31(self.e31(x))))))\n",
    "        \n",
    "        \n",
    "        #### Classifier\n",
    "        # xbn = x\n",
    "        # xclass = xbn.reshape(xbn.size(0),-1)\n",
    "        # xclass = self.relu(self.fc1(xclass))\n",
    "        # xclass = self.relu(self.fc2(xclass))\n",
    "        # xclass = self.relu(self.fc3(xclass))\n",
    "        # xclass = (self.fc4(xclass))\n",
    "        # xclass = self.softmax(xclass) \n",
    "        \n",
    "        \n",
    "        \n",
    "        # out = x.reshape(x.size(0), -1)\n",
    "\n",
    "\n",
    "        # decoder\n",
    "        x = torch.cat((self.upconv3(x), x2), dim=1)\n",
    "        x = self.relu(self.bn24(self.d22(self.relu(self.bn23(self.d21(x))))))\n",
    "        \n",
    "        x = torch.cat((self.upconv2(x), x1), dim=1)\n",
    "        x = self.relu(self.bn14(self.d12(self.relu(self.bn13(self.d11(x))))))\n",
    "\n",
    "        x = torch.cat((self.upconv1(x), x0), dim=1)\n",
    "        x = self.relu(self.bn04(self.d02(self.relu(self.bn03(self.d01(x))))))\n",
    "\n",
    "        out = self.out(x)\n",
    "#         out=self.sig(out)\n",
    "#         return out\n",
    "\n",
    "        return  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.5):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "       \n",
    "    def forward(self, pred_logits, target):\n",
    "        pred = pred_logits.sigmoid()\n",
    "        ce = F.binary_cross_entropy_with_logits(pred_logits, target, reduction = 'none')\n",
    "        alpha = target*self.alpha + (1. - target )*(1. - self.alpha)\n",
    "        pt = torch.where(target == 1, pred, 1- pred)\n",
    "        return alpha* (1. - pt)**self.gamma * ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e01): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn01): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e02): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn02): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e11): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn11): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn12): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn21): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn22): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn31): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn32): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d21): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn23): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn24): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d11): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn13): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn14): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d01): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn03): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (d02): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn04): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (fc1): Linear(in_features=40960, out_features=2048, bias=True)\n",
       "  (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "  (fc3): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=3, bias=True)\n",
       "  (softmax): Softmax(dim=3)\n",
       "  (out): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (relu): ReLU()\n",
       "  (sig): Sigmoid()\n",
       "  (sm): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initializing the network\n",
    "device = 'cuda:3'\n",
    "model = UNet(32).float().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs=15\n",
    "learning_rate=0.001\n",
    "# criterion1 = FocalLoss()\n",
    "# criterion2 = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "# criterion1 = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 256, 1024]),\n",
       " torch.Size([1, 3, 256, 1024]),\n",
       " torch.Size([1, 3, 256, 1024]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, target.shape,output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 3; 23.68 GiB total capacity; 11.37 GiB already allocated; 9.62 MiB free; 11.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lambda2/carla_garage/Unet.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data, target \u001b[39m=\u001b[39m train_dataloader\u001b[39m.\u001b[39mdataset[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output\u001b[39m=\u001b[39mmodel(data\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49mto(device))\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m#backward\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/lambda2/carla_garage/Unet.ipynb Cell 17\u001b[0m line \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn24(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md22(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn23(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md21(x))))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupconv2(x), x1), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn14(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md12(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn13(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md11(x))))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupconv1(x), x0), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X23sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn04(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md02(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn03(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md01(x))))))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2438\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2436\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2438\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2439\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2440\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 3; 23.68 GiB total capacity; 11.37 GiB already allocated; 9.62 MiB free; 11.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "data, target = train_dataloader.dataset[0:1]\n",
    "output=model(data.float().to(device)).float().to(device)\n",
    "loss = criterion(output, target)\n",
    "#backward\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "        \n",
    "        # loss = criterion(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 3; 23.68 GiB total capacity; 10.40 GiB already allocated; 145.62 MiB free; 11.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/lambda2/carla_garage/Unet.ipynb Cell 18\u001b[0m line \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m target\u001b[39m=\u001b[39mtarget\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#forward\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m output\u001b[39m=\u001b[39mmodel(data)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#backward\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/lambda2/carla_garage/Unet.ipynb Cell 18\u001b[0m line \u001b[0;36mUNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn14(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md12(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn13(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md11(x))))))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m         x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupconv1(x), x0), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn04(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md02(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn03(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49md01(x))))))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(x)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m#         out=self.sig(out)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/lambda2/carla_garage/Unet.ipynb#X24sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m#         return out\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 3; 23.68 GiB total capacity; 10.40 GiB already allocated; 145.62 MiB free; 11.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#Train the network\n",
    "loss_ls=[]\n",
    "for epoch in range(n_epochs):\n",
    "    loss_sum=0\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "\n",
    "\n",
    "        data=data.float().to(device)\n",
    "        target=target.float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "        #forward\n",
    "        output=model(data).float().to(device)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum=loss_sum+loss\n",
    "        \n",
    "    loss_ls.append(loss_sum)\n",
    "    print('epoch:',epoch, '\\n', loss_sum)\n",
    "    PATH = 'm'+str(epoch)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss_sum}, PATH)\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_ls = torch.tensor(loss_ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet(32).float()\n",
    "# model.eval()\n",
    "# PATH = 'm'+str(epoch)\n",
    "# checkpoint = torch.load(PATH)\n",
    "# # model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test loss\n",
    "loss_test_ls=[]\n",
    "for epoch in range(n_epochs):\n",
    "    model = UNet(32).float()\n",
    "    PATH = 'm'+str(epoch)\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    loss_sum=0\n",
    "        \n",
    "    \n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(test_dataloader):\n",
    "        \n",
    "        data=data.float().to(device)\n",
    "       \n",
    "        target=target.float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "        #forward\n",
    "        output=model(data)\n",
    "        out_decoder = output['out_decoder'].float().to(device)\n",
    "        tclass = output['label'].float().to(device)\n",
    "#         loss=criterion1(out_decoder, data)+criterion2(tclass, target)\n",
    "#        loss1 = loss1.mean()\n",
    "\n",
    "        loss = criterion1(tclass, target)+criterion2(out_decoder, data).mean()\n",
    "        loss_sum=loss_sum+loss\n",
    "#         loss = criterion2(out_decoder, data)\n",
    "        #backward\n",
    "        \n",
    "    loss_test_ls.append(loss_sum)\n",
    "    print(loss_sum)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_test_ls = torch.tensor(loss_test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_test_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_ls = []\n",
    "for epoch in range(n_epochs):\n",
    "    PATH = 'm'+str(epoch)\n",
    "    checkpoint = torch.load(PATH)\n",
    "    train_loss_ls.append(checkpoint[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(data)\n",
    "output['out_decoder'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2(out_decoder, data).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_adrs=8609\n",
    "output=model((train_data[test_adrs:test_adrs+1,:,:,:]).to(device).float())\n",
    "out_decoder = output['out_decoder'].cpu().detach()\n",
    "out_decoder = torch.sigmoid(out_decoder)\n",
    "# out_decoder = torch.abs(out_decoder)\n",
    "# out_decoder = out_decoder/out_decoder.max()\n",
    "tclass = output['label'].cpu().detach()\n",
    "outplot = np.zeros([80,128,3])\n",
    "outplot[:,:,0] = np.array(out_decoder[0][0])\n",
    "outplot[:,:,1] = np.array(out_decoder[0][1])\n",
    "outplot[:,:,2] = np.array(out_decoder[0][2])\n",
    "img = (train_data[test_adrs])\n",
    "inplot = np.zeros([80,128,3])\n",
    "inplot[:,:,0] = np.array(img[0])\n",
    "inplot[:,:,1] = np.array(img[1])\n",
    "inplot[:,:,2] = np.array(img[2])\n",
    "fig, ax = plt.subplots(2, sharex='row', sharey='row',figsize=(10,10))\n",
    "\n",
    "im=ax[0].imshow(outplot)\n",
    "# fig.colorbar(im,cax=ax[0])\n",
    "im1=ax[1].imshow(inplot)\n",
    "# fig.colorbar(im1,cax=ax[1])\n",
    "print('predication:',F.sigmoid(tclass))\n",
    "print('label:', label_train[test_adrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_adrs=95\n",
    "output=model((test_data[test_adrs:test_adrs+1,:,:,:]).to(device).float())\n",
    "out_decoder = output['out_decoder'].cpu().detach()\n",
    "tclass = output['label'].cpu().detach()\n",
    "outplot = np.zeros([80,128,3])\n",
    "outplot[:,:,0] = np.array(out_decoder[0][0])\n",
    "outplot[:,:,1] = np.array(out_decoder[0][1])\n",
    "outplot[:,:,2] = np.array(out_decoder[0][2])\n",
    "img = (test_data[test_adrs])\n",
    "inplot = np.zeros([80,128,3])\n",
    "inplot[:,:,0] = np.array(img[0])\n",
    "inplot[:,:,1] = np.array(img[1])\n",
    "inplot[:,:,2] = np.array(img[2])\n",
    "fig, ax = plt.subplots(2, sharex='row', sharey='row',figsize=(10,10))\n",
    "\n",
    "im=ax[0].imshow(outplot)\n",
    "# fig.colorbar(im,cax=ax[0])\n",
    "im1=ax[1].imshow(inplot)\n",
    "# fig.colorbar(im1,cax=ax[1])\n",
    "print('predication:',F.sigmoid(tclass))\n",
    "print('label:', label_train[test_adrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_adrs=2\n",
    "output=model(rs(train_data[test_adrs:test_adrs+1,:,:,:]).to(device).float())\n",
    "out_decoder = output['out_decoder'].cpu().detach()\n",
    "tclass = output['label'].cpu().detach()\n",
    "outplot = np.zeros([80,128,3])\n",
    "outplot[:,:,0] = np.array(out_decoder[0][0])\n",
    "outplot[:,:,1] = np.array(out_decoder[0][1])\n",
    "outplot[:,:,2] = np.array(out_decoder[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_decoder.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(out_decoder[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outplot = np.zeros([80,128,3])\n",
    "outplot[:,:,0] = np.array(out_decoder[0][0])\n",
    "outplot[:,:,1] = np.array(out_decoder[0][1])\n",
    "outplot[:,:,2] = np.array(out_decoder[0][2])\n",
    "plt.imshow(outplot)\n",
    "# outplot = torch.tensor(outplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Metrics\n",
    "# device = 'cuda:3'\n",
    "# PATH = 'm'+str('14')\n",
    "# checkpoint = torch.load(PATH)\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prd_dete(pred_label, number_class):\n",
    "    #####Determining the pred_label\n",
    "    l = np.eye(number_class)\n",
    "    d = []\n",
    "    for i in range(number_class):\n",
    "        d.append(np.linalg.norm(np.array(pred_label.cpu().detach()) - l[i,:]))\n",
    "         \n",
    "\n",
    "    d = np.array(d)\n",
    "    return np.argmin(d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_dete(torch.tensor([[0.99, 1, 0.3]]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Metrics\n",
    "def conf_matrix(inp_tens):\n",
    "\n",
    "    n_class = np.array([0,0,0])\n",
    "    for it in range(len(inp_tens)):\n",
    "\n",
    "        output=model((inp_tens[it:it+1,:,:,:]).to(device).float())\n",
    "        out_decoder = output['out_decoder'].cpu().detach()\n",
    "        out_decoder = torch.sigmoid(out_decoder)\n",
    "        # out_decoder = torch.abs(out_decoder)\n",
    "        # out_decoder = out_decoder/out_decoder.max()\n",
    "        tclass = output['label'].cpu().detach()\n",
    "        ncl = prd_dete(tclass,3)\n",
    "        n_class[ncl] = n_class[ncl] +1\n",
    "            \n",
    "    return n_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_train_ratio = 0.9\n",
    "# pos_imgs_train = pos_imgs[0:int(np.floor(test_train_ratio*len(pos_imgs)))]\n",
    "# pos_imgs_test = pos_imgs[int(np.floor(test_train_ratio*len(pos_imgs))):]\n",
    "# neg_imgs_train = neg_imgs[0:int(np.floor(test_train_ratio*len(neg_imgs)))]\n",
    "# neg_imgs_test = neg_imgs[int(np.floor(test_train_ratio*len(neg_imgs))):]\n",
    "# pos_imgs_train_label = torch.ones((len(pos_imgs_train),1))\n",
    "# pos_imgs_test_label = torch.ones((len(pos_imgs_test),1))\n",
    "# neg_imgs_train_label = torch.zeros((len(neg_imgs_train),1))\n",
    "# neg_imgs_test_label = torch.zeros((len(neg_imgs_test),1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(pos_imgs_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(neg_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(noev_imgs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(pos_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(neg_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix(noev_imgs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
